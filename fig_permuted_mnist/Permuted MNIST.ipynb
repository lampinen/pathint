{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) 2017 Ben Poole & Friedemann Zenke\n",
    "# MIT License -- see LICENSE for details\n",
    "# \n",
    "# This file is part of the code to reproduce the core results of:\n",
    "# Zenke, F., Poole, B., and Ganguli, S. (2017). Continual Learning Through\n",
    "# Synaptic Intelligence. In Proceedings of the 34th International Conference on\n",
    "# Machine Learning, D. Precup, and Y.W. Teh, eds. (International Convention\n",
    "# Centre, Sydney, Australia: PMLR), pp. 3987â€“3995.\n",
    "# http://proceedings.mlr.press/v70/zenke17a.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "graph_replace = tf.contrib.graph_editor.graph_replace\n",
    "\n",
    "import sys, os\n",
    "sys.path.extend([os.path.expanduser('..')])\n",
    "from pathint import utils\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data params\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "\n",
    "# Network params\n",
    "n_hidden_units = 2000\n",
    "activation_fn = tf.nn.relu\n",
    "\n",
    "# Optimization params\n",
    "batch_size = 256\n",
    "epochs_per_task = 20 \n",
    "learning_rate=1e-3\n",
    "xi = 0.1\n",
    "\n",
    "# Reset optimizer after each age\n",
    "reset_optimizer = False\n",
    "\n",
    "# replay prior tasks while training new\n",
    "replay_all_prior = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_tasks = 10\n",
    "full_datasets, final_test_datasets = utils.construct_permute_mnist(num_tasks=n_tasks)\n",
    "# training_datasets, validation_datasets = utils.mk_training_validation_splits(full_datasets, split_fractions=(0.9, 0.1))\n",
    "training_datasets = full_datasets\n",
    "validation_datasets = final_test_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct network, loss, and updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden_units, activation=activation_fn, input_dim=input_dim))\n",
    "model.add(Dense(n_hidden_units, activation=activation_fn))\n",
    "model.add(Dense(output_dim, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathint import protocols\n",
    "from pathint.optimizers import KOOptimizer\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.callbacks import Callback\n",
    "from pathint.keras_utils import LossHistory\n",
    "\n",
    "\n",
    "#protocol_name, protocol = protocols.PATH_INT_PROTOCOL(omega_decay='sum', xi=xi)\n",
    "# protocol_name, protocol = protocols.FISHER_PROTOCOL(omega_decay='sum')\n",
    "protocol_name, protocol = protocols.UNREGULARIZED_PROTOCOL(replay_prior=replay_all_prior)\n",
    "opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "opt_name = 'adam'\n",
    "# opt = SGD(lr=learning_rate)\n",
    "# opt_name = 'sgd'\n",
    "oopt = KOOptimizer(opt, model=model, **protocol)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=oopt, metrics=['accuracy'])\n",
    "\n",
    "history = LossHistory()\n",
    "callbacks = [history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_prefix = \"data_%s_opt%s_lr%.2e_bs%i_ep%i_tsks%i\"%(protocol_name, opt_name, learning_rate, batch_size, epochs_per_task, n_tasks)\n",
    "datafile_name = \"%s.pkl.gz\"%(file_prefix)\n",
    "fisher_file_name = \"%s_fishers.pkl.gz\"%(file_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diag_vals = dict()\n",
    "all_evals = dict()\n",
    "# all_evals = utils.load_zipped_pickle(datafile_name)\n",
    "# returns empty dict if file not found\n",
    "\n",
    "def run_fits(cvals, training_data, valid_data, eval_on_train_set=False):\n",
    "    fishers = {cval_: {w.name: [] for w in oopt.model.weights} for cval_ in cvals}\n",
    "    for cidx, cval_ in enumerate(cvals):\n",
    "        fs = []\n",
    "        evals = []\n",
    "        abs_grad_importances = []\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        cstuffs = []\n",
    "        cval = cval_\n",
    "        print( \"setting cval\")\n",
    "        oopt.set_strength(cval)\n",
    "        print(\"cval is %e\"%sess.run(oopt.lam))\n",
    "        \n",
    "        print(fishers)\n",
    "        for age, tidx in enumerate(range(n_tasks)):\n",
    "            print(\"Age %i, cval is=%f\"%(age,cval))\n",
    "            oopt.set_nb_data(len(training_data[tidx][0]))\n",
    "            print(training_data[tidx][0])\n",
    "            if replay_all_prior:\n",
    "                this_xdata = np.concatenate([vec for x in training_data[:tidx+1] for vec in x[0]])\n",
    "                this_ydata = np.concatenate([vec for x in training_data[:tidx+1] for vec in x[1]])\n",
    "                print(this_xdata)\n",
    "                print(this_ydata)\n",
    "            else:\n",
    "                this_xdata = training_data[tidx][0]\n",
    "                this_ydata = training_data[tidx][1]\n",
    "            stuffs = model.fit(this_xdata, this_ydata, batch_size, epochs_per_task, callbacks=callbacks,\n",
    "                              verbose=0)\n",
    "            \n",
    "            oopt.update_task_metrics(training_data[tidx][0], training_data[tidx][1], batch_size)\n",
    "            these_fishers = oopt.get_numvals_dict('task_fisher')\n",
    "            for k, v in these_fishers.items():\n",
    "                fishers[cval_][k].append(v)\n",
    "            oopt.update_task_vars()\n",
    "            ftask = []\n",
    "            for j in range(n_tasks):\n",
    "                if eval_on_train_set:\n",
    "                    f_ = model.evaluate(training_data[j][0], training_data[j][1], batch_size, verbose=0)\n",
    "                else:\n",
    "                    f_ = model.evaluate(valid_data[j][0], valid_data[j][1], batch_size, verbose=0)\n",
    "                ftask.append(np.mean(f_[1]))\n",
    "            evals.append(ftask)\n",
    "            cstuffs.append(stuffs)\n",
    "            \n",
    "            # Re-initialize optimizer variables\n",
    "            if reset_optimizer:\n",
    "                oopt.reset_optimizer()\n",
    "\n",
    "        # diag_vals[cval_] = oopt.get_numvals('omega')\n",
    "        evals = np.array(evals)\n",
    "        all_evals[cval_] = evals\n",
    "        \n",
    "        # backup all_evals to disk\n",
    "        utils.save_zipped_pickle(all_evals, datafile_name)\n",
    "        utils.save_zipped_pickle(fishers, fisher_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "cvals = [0]\n",
    "print(cvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cval\n",
      "cval is 0.000000e+00\n",
      "{0: {'dense_3/kernel:0': [], 'dense_2/kernel:0': [], 'dense_3/bias:0': [], 'dense_1/bias:0': [], 'dense_2/bias:0': [], 'dense_1/kernel:0': []}}\n",
      "Age 0, cval is=0.000000\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (784,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8db127f9fd56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%capture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_fits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-6062bdad831f>\u001b[0m in \u001b[0;36mrun_fits\u001b[0;34m(cvals, training_data, valid_data, eval_on_train_set)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mthis_ydata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             stuffs = model.fit(this_xdata, this_ydata, batch_size, epochs_per_task, callbacks=callbacks,\n\u001b[0;32m---> 32\u001b[0;31m                               verbose=0)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_task_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (784,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "run_fits(cvals, training_datasets, validation_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup all_evals to disk\n",
    "# all_evals = dict() # uncomment to delete on disk\n",
    "utils.save_zipped_pickle(all_evals, datafile_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o = oopt.get_numvals_list('omega')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('cool') \n",
    "cNorm  = colors.Normalize(vmin=-4, vmax=np.log(np.max(list(all_evals.keys()))))\n",
    "# cNorm  = colors.Normalize()\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "print(scalarMap.get_clim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(14, 4))\n",
    "axs = [subplot(1,n_tasks+1,1)]#, None, None]\n",
    "for i in range(1, n_tasks + 1):\n",
    "    axs.append(subplot(1, n_tasks+1, i+1, sharex=axs[0], sharey=axs[0]))\n",
    "    \n",
    "keys = list(all_evals.keys())\n",
    "sorted_keys = np.sort(keys)\n",
    "\n",
    "for cval in sorted_keys:\n",
    "    evals = all_evals[cval]\n",
    "    for j in range(n_tasks):\n",
    "        colorVal = scalarMap.to_rgba(np.log(cval))\n",
    "        axs[j].plot(evals[:, j], c=colorVal)#, label=\"t%d, c%g\"%(j, cval))\n",
    "    label = \"c=%g\"%cval\n",
    "    average = evals.mean(1)\n",
    "    axs[-1].plot(average, c=colorVal, label=label)\n",
    "    \n",
    "for i, ax in enumerate(axs):\n",
    "    ax.legend(bbox_to_anchor=(1.0,1.0))\n",
    "    ax.set_title((['task %d'%j for j in range(n_tasks)] + ['average'])[i])\n",
    "gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cval in sorted_keys:\n",
    "    stuff = []\n",
    "    for i in range(len(all_evals[cval])):#n_tasks):\n",
    "        stuff.append(all_evals[cval][i][:i+1].mean())\n",
    "    plot(range(1,n_tasks+1), stuff, 'o-', label=\"c=%g\"%cval)\n",
    "axhline(all_evals[cval][0][0], linestyle='--', color='k')\n",
    "    \n",
    "xlabel('Number of tasks')\n",
    "ylabel('Fraction correct')\n",
    "legend(loc='best')\n",
    "ylim(0.9, 1.02)\n",
    "xlim(0.5, 10.5)\n",
    "grid('on')\n",
    "\n",
    "savefig(\"%s.pdf\"%(file_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
